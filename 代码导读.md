# IKE 项目代码详细导读

## 📚 项目背景

这个项目研究的核心问题是：**能否通过上下文学习（In-Context Learning）来编辑大型语言模型中的事实知识？**

简单来说，就是通过给模型看一些示例，让它学会新的知识或修正错误的知识。

---

## 🗺️ 项目结构

```
IKE/
├── clean_paraphrase.py      # 数据清理脚本
├── encode_facts.py           # 事实编码脚本
├── semantic_search.py        # 语义搜索脚本
├── icl.py                    # 核心实验代码
├── counterfact.json          # CounterFact数据集
├── corpus_idx.txt            # 预计算的相似样本索引
├── requirements.txt          # Python依赖包
├── README.md                 # 项目说明文档
└── 快速开始指南.md          # 快速上手指南
```

---

## 📖 数据集说明

### CounterFact数据集结构

```json
{
  "case_id": 1234,
  "requested_rewrite": {
    "subject": "Elon Musk",                    // 主语
    "prompt": "{} works at",                   // 提示模板
    "target_new": {
      "str": "Microsoft"                       // 新答案（反事实）
    },
    "target_true": {
      "str": "Tesla"                           // 真实答案
    },
    "relation_id": "P937"                      // 关系ID
  },
  "paraphrase_prompts": [                      // 改述提示（不同说法）
    "The employer of {} is",
    "{} is employed by"
  ],
  "neighborhood_prompts": [                    // 邻域提示（相关问题）
    "Tesla was founded by",
    "SpaceX is led by"
  ]
}
```

**数据划分**：
- **前2000条**：测试集（用于评估）
- **后续数据**：示例库（用于构建上下文）

---

## 🔍 代码文件详解

### 1. clean_paraphrase.py - 数据清理

**目的**：清理 `paraphrase_prompts` 中的多余前缀，统一格式。

**主要逻辑**：
1. 读取 `counterfact.json`
2. 找到每个改述提示中主语（subject）的位置
3. 删除主语之前的句号、换行符等无关内容
4. 删除 "Category" 等特殊前缀
5. 保存清理后的数据

**示例**：
```
清理前: "In the history of the world. Elon Musk works at"
清理后: "Elon Musk works at"
```

**运行方式**：
```bash
python clean_paraphrase.py
```

---

### 2. encode_facts.py - 事实编码

**目的**：将所有事实转换为语义向量，方便后续查找相似示例。

**核心技术**：
- 使用 `SentenceTransformer` 模型（all-MiniLM-L6-v2）
- 将文本转换为384维的向量

**编码格式**：
```
New Fact: Elon Musk works at Microsoft
Prompt: Elon Musk works at
```

**输出文件**：
- `embeddings.pkl`：包含所有句子、向量、主语的pickle文件

**运行方式**：
```bash
python encode_facts.py
```

**注意**：这个脚本运行时间较长（几小时），需要较好的网络下载模型。

---

### 3. semantic_search.py - 语义搜索

**目的**：为每个测试样本找到最相似的32个示例。

**算法**：
1. 加载 `embeddings.pkl`
2. 对每个测试样本，计算与所有示例的余弦相似度
3. 选择最相似的32个示例（排除同一主语的示例）
4. 保存到 `corpus_idx.txt`

**输出格式**（corpus_idx.txt）：
```
2156 2234 2567 2890 ...  # 第1个测试样本的相似示例索引
2045 2178 2423 2756 ...  # 第2个测试样本的相似示例索引
...
```

**运行方式**：
```bash
python semantic_search.py
```

**注意**：项目已包含预计算的 `corpus_idx.txt`，通常不需要重新运行。

---

### 4. icl.py - 核心实验代码（最重要！）

这是项目的核心，让我详细拆解每个部分：

#### 4.1 命令行参数

```python
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--model_name', type=str, default='EleutherAI/gpt-j-6B')
```

**支持的模型**：
- `gpt2-xl`：1.5B参数，需要约6GB显存
- `EleutherAI/gpt-neo-1.3B`：1.3B参数，需要约16GB显存
- `EleutherAI/gpt-j-6B`：6B参数，需要约24GB显存（默认）
- `EleutherAI/gpt-neox-20b`：20B参数，需要约40GB显存

#### 4.2 构建上下文示例 - construct_icl_examples()

这是最核心的函数！理解它就理解了整个项目。

**参数**：
- `idx`：测试样本的索引
- `demos`：示例库（2000条后的数据）

**返回**：
- 一个包含32个示例的列表

**示例类型（order）**：
- `0`：原始提示 + 新答案
  ```
  New Fact: Bill Gates works at Apple
  Prompt: Bill Gates works at Apple
  ```
- `1`：改述提示 + 新答案
  ```
  New Fact: Bill Gates works at Apple
  Prompt: The employer of Bill Gates is Apple
  ```
- `2`：邻域提示 + 旧答案（正确答案）
  ```
  New Fact: Bill Gates works at Apple
  Prompt: Microsoft was founded by Bill Gates
  ```

**为什么要混合三种类型？**
- 让模型学会：看到新事实后，应该输出新答案
- 同时保持：对于其他相关问题，仍然输出正确答案

#### 4.3 评估函数 - icl_lm_eval()

**输入**：
- `model`：语言模型
- `tokenizer`：分词器
- `icl_examples`：上下文示例列表
- `targets`：候选答案列表（通常是 [新答案, 旧答案]）
- `x`：要评估的提示

**输出**：
- 每个候选答案的困惑度（perplexity）列表

**困惑度（PPL）的含义**：
- 衡量模型对某个答案的"惊讶程度"
- **PPL 越低 = 模型越确信这个答案**
- 公式：`PPL = exp(loss)`

**判断标准**：
```python
prob_new = 1 / ppl_new
prob_old = 1 / ppl_old

if prob_new > prob_old:
    # 模型更倾向于新答案，编辑成功！
    success_cnt += 1
```

#### 4.4 主实验循环

**流程图**：
```
对每个测试样本：
  ├─ 1. 构建上下文示例（32个相似示例）
  ├─ 2. 测试原始提示
  │    └─ 模型能输出新答案吗？
  ├─ 3. 测试改述提示（多个不同说法）
  │    └─ 换个说法，模型还能输出新答案吗？
  └─ 4. 测试邻域提示（相关问题）
       └─ 问相关问题时，模型会输出正确答案吗？
```

**评价指标**：

1. **原始提示效果（Efficacy）**：
   - `orig_success_cnt / orig_total_cnt`
   - 模型能否在原始提示下输出新答案

2. **泛化能力（Generalization）**：
   - `para_success_cnt / para_total_cnt`
   - 换个说法，模型还能输出新答案吗？

3. **局部性（Locality）**：
   - `success_cnt / total_cnt`
   - 问相关问题时，模型是否仍然输出正确的旧答案？

4. **效果强度（Magnitude）**：
   - `magnitude / total_cnt`
   - 新答案概率 - 旧答案概率的平均值
   - 越高说明编辑效果越明显

#### 4.5 输出解读

运行时会看到这样的输出：
```
0   0   0   0.0      0   0.0      0   0.0
10  45  68  0.125    38  0.089    8   0.156
20  92  138 0.142    78  0.095    17  0.168
```

**每列含义**：
1. 处理的样本数
2. 邻域提示成功次数
3. 邻域提示总次数
4. 邻域提示平均得分（局部性）
5. 改述提示成功次数
6. 改述提示平均得分（泛化能力）
7. 原始提示成功次数
8. 原始提示平均得分（有效性）

**最终输出**：
```python
print(
    success_cnt/total_cnt,           # 局部性成功率
    magnitude/total_cnt,             # 局部性强度
    para_success_cnt/para_total_cnt, # 泛化成功率
    para_magnitude/para_total_cnt,   # 泛化强度
    orig_success_cnt/orig_total_cnt, # 有效性成功率
    orig_magnitude/orig_total_cnt    # 有效性强度
)
```

---

## 🎯 核心算法：上下文学习（ICL）

### 什么是上下文学习？

给模型看一些示例，让它通过模仿来完成任务，**无需更新模型参数**。

### 本项目的ICL示例格式

```
[示例1]
New Fact: Bill Gates works at Apple
Prompt: Bill Gates works at Apple

[示例2]
New Fact: Steve Jobs founded Google
Prompt: What company did Steve Jobs found? Google

...（共32个示例）...

[待测试]
New Fact: Elon Musk works at Microsoft
Prompt: Elon Musk works at
```

模型会基于前面的示例，推断出应该输出 "Microsoft"。

### 为什么要用语义相似的示例？

**直觉**：
- 相似的问题应该有相似的处理方式
- 如果示例都是关于"人物工作单位"的，模型更容易学会这个模式

**实验验证**：
- 使用相似示例 > 使用随机示例
- 相似度越高，效果越好

---

## 🔬 实验设计的三个维度

### 1. Efficacy（有效性）
**问题**：模型能学会新知识吗？
**测试**：直接问原始问题，看模型是否输出新答案
**示例**：
```
New Fact: Elon Musk works at Microsoft
Prompt: Elon Musk works at _____
期望输出: Microsoft ✓
```

### 2. Generalization（泛化性）
**问题**：换个说法，模型还能答对吗？
**测试**：用改述提示测试
**示例**：
```
New Fact: Elon Musk works at Microsoft
Prompt: The employer of Elon Musk is _____
期望输出: Microsoft ✓
```

### 3. Locality（局部性）
**问题**：学会新知识后，会不会影响其他知识？
**测试**：问相关但不同的问题
**示例**：
```
New Fact: Elon Musk works at Microsoft
Prompt: Tesla was founded by _____
期望输出: Elon Musk ✓（而不是被Microsoft影响）
```

---

## 🚀 运行指南

### 最简运行流程

```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 下载数据集
wget https://rome.baulab.info/data/dsets/counterfact.json

# 3. 清理数据
python clean_paraphrase.py

# 4. 运行实验（使用默认模型）
python icl.py

# 或使用小模型快速测试
python icl.py --model_name gpt2-xl
```

### 自定义运行

```bash
# 使用不同的模型
python icl.py --model_name EleutherAI/gpt-neo-1.3B

# 设置随机种子
python icl.py --seed 123

# 组合参数
python icl.py --model_name gpt2-xl --seed 42
```

---

## 💡 代码技巧与细节

### 1. 为什么要反转示例顺序？

```python
icl_examples.reverse()
```

**原因**：语言模型对**最近的示例**更敏感，反转后最相似的示例会出现在最后，效果更好。

### 2. 为什么用困惑度而不是直接生成？

**困惑度方法**：
- 可以精确比较多个候选答案
- 避免生成随机性的影响
- 更适合评估知识编辑效果

**生成方法**：
- 可能生成意外的答案
- 难以量化比较

### 3. 为什么设置 max_length=1024？

```python
encodings = tokenizer(full_text, return_tensors='pt', 
                      truncation=True, max_length=1024)
```

**原因**：
- GPT系列模型的上下文窗口限制
- 32个示例 + 测试提示 ≈ 800-1000 tokens
- 1024是一个平衡点（足够容纳示例，又不会超出限制）

### 4. 为什么要随机打乱order？

```python
random.shuffle(order)
```

**原因**：
- 避免示例顺序的偏差
- 增加实验的随机性和鲁棒性

---

## 🐛 常见问题与解决

### 1. CUDA out of memory

**原因**：模型太大，显存不足
**解决**：
```bash
# 使用更小的模型
python icl.py --model_name gpt2-xl  # 需要6GB显存
```

### 2. 模型下载失败

**原因**：网络问题或HuggingFace访问受限
**解决**：
- 设置镜像：`export HF_ENDPOINT=https://hf-mirror.com`
- 手动下载模型到 `~/.cache/huggingface/`

### 3. 运行速度慢

**这是正常的！**
- 2000个样本 × 平均10个提示 = 20000次推理
- 每次推理 ≈ 1-3秒
- 总共需要数小时

**加速方法**：
- 使用更小的模型
- 只测试前100个样本（修改代码）

### 4. 困惑度为inf或nan

**原因**：某些答案的概率极低
**解决**：代码中已有保护机制（`1e-12`）

---

## 📊 结果分析示例

假设运行后得到：
```
局部性: 0.85 (成功率) 0.23 (强度)
泛化性: 0.72 (成功率) 0.18 (强度)
有效性: 0.91 (成功率) 0.31 (强度)
```

**解读**：
- ✅ **有效性很好**（0.91）：模型成功学会了91%的新知识
- ✅ **局部性较好**（0.85）：85%的相关问题仍然答对
- ⚠️ **泛化性一般**（0.72）：换个说法后，只有72%能答对

**改进方向**：
- 增加改述类型的示例（type 1）
- 使用更多样化的改述提示

---

## 🎓 扩展阅读

### 相关论文
- "Can We Edit Factual Knowledge by In-Context Learning?"（本项目论文）
- "Language Models are Few-Shot Learners"（GPT-3论文，提出ICL）
- "Locating and Editing Factual Associations in GPT"（ROME方法）

### 相关技术
- **In-Context Learning**：通过示例学习
- **Prompt Engineering**：提示词工程
- **Knowledge Editing**：知识编辑

---

## 📝 项目改进建议

1. **添加断点续传功能**
   - 保存中间结果
   - 支持从中断处继续运行

2. **优化显存使用**
   - 使用梯度检查点
   - 支持8-bit量化

3. **增强错误处理**
   - 捕获异常情况
   - 输出详细的错误信息

4. **可视化结果**
   - 生成图表
   - 分析不同类型关系的表现

---

## 🎉 总结

这个项目研究了一个有趣的问题：**能否不更新模型参数，仅通过提供示例来编辑模型的知识？**

**核心发现**：
- ✅ 可行！上下文学习确实能编辑知识
- ⚠️ 但效果有限，泛化性和局部性需要权衡
- 💡 相似示例的选择至关重要

**实用价值**：
- 无需重新训练模型
- 可以快速修正错误知识
- 适合小规模知识更新

希望这份导读能帮助你深入理解这个项目！

