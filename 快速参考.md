# 🚀 IKE 项目快速参考卡片

这是一份简洁的参考卡片，帮助你快速回顾项目的核心要点。

---

## 📋 项目核心

### 一句话总结
通过给语言模型看示例，让它学会新知识，**无需重新训练**。

### 核心流程
```
准备数据 → 编码事实 → 查找相似示例 → 运行实验 → 分析结果
```

---

## 📁 文件速查

| 文件名 | 作用 | 运行时间 |
|--------|------|----------|
| `clean_paraphrase.py` | 清理数据 | 1-2分钟 |
| `encode_facts.py` | 生成语义向量 | 1-3小时 |
| `semantic_search.py` | 查找相似示例 | 1-2小时 |
| `icl.py` | **主实验**（最重要） | 2-8小时 |
| `corpus_idx.txt` | 预计算的索引 | - |
| `counterfact.json` | 数据集 | - |

---

## 🎯 核心概念

### 上下文学习（ICL）
给模型看示例 → 模型学会模式 → 应用到新问题

### 三个评估指标

| 指标 | 英文 | 问题 | 测试方法 |
|------|------|------|----------|
| 有效性 | Efficacy | 能学会吗？ | 直接问原始问题 |
| 泛化性 | Generalization | 换个说法还对吗？ | 用改述提示测试 |
| 局部性 | Locality | 会影响其他知识吗？ | 用邻域提示测试 |

### 三种示例类型

| 类型 | 代码 | 格式 | 作用 |
|------|------|------|------|
| 原始 | o=0 | 原始提示 + 新答案 | 教标准答案 |
| 改述 | o=1 | 改述提示 + 新答案 | 增强泛化 |
| 邻域 | o=2 | 邻域提示 + 旧答案 | 保持局部性 |

---

## 💻 常用命令

### 运行实验
```bash
# 使用默认模型（需要24GB显存）
python icl.py

# 使用小模型（需要6GB显存）
python icl.py --model_name gpt2-xl

# 使用中等模型（需要16GB显存）
python icl.py --model_name EleutherAI/gpt-neo-1.3B
```

### 数据准备
```bash
# 下载数据集
wget https://rome.baulab.info/data/dsets/counterfact.json

# 清理数据
python clean_paraphrase.py
```

---

## 🔍 核心函数

### construct_icl_examples(idx, demos)
**作用**：构建32个上下文示例  
**输入**：测试样本索引、示例库  
**输出**：示例列表  
**关键**：混合三种类型，反转顺序

### icl_lm_eval(model, tokenizer, icl_examples, targets, x)
**作用**：评估模型对每个答案的偏好  
**输入**：模型、分词器、示例、候选答案、提示  
**输出**：困惑度列表  
**关键**：困惑度越低 = 模型越确信

---

## 📊 输出解读

### 运行时输出
```
[样本数] [邻域成功] [邻域总数] [邻域得分] [改述成功] [改述得分] [原始成功] [原始得分]
```

### 示例
```
10 45 68 0.125 38 0.089 8 0.156
```
- 处理了10个样本
- 邻域：68次测试，成功45次
- 改述：成功38次
- 原始：成功8次

---

## 🐛 常见问题

| 问题 | 原因 | 解决方案 |
|------|------|----------|
| CUDA out of memory | 显存不足 | 使用更小的模型 `--model_name gpt2-xl` |
| 下载模型慢 | 网络问题 | 使用镜像 `export HF_ENDPOINT=https://hf-mirror.com` |
| 运行时间长 | 正常现象 | 2000个样本需要数小时 |
| 找不到文件 | 路径错误 | 确保在项目目录下运行 |

---

## 📚 学习资源

| 资源 | 适合人群 | 阅读时间 |
|------|----------|----------|
| `快速开始指南.md` | 新手运行 | 10分钟 |
| `学习路径.md` | 系统学习 | 2-3天 |
| `代码导读.md` | 深入理解 | 3-5小时 |
| `README.md` | 完整参考 | 20分钟 |

---

## 🎯 关键数字

- **2000**：测试样本数量
- **32**：每个测试使用的示例数量
- **3**：示例类型数量（原始、改述、邻域）
- **1024**：最大token长度

---

## 🔑 关键概念速记

### 困惑度（PPL）
- 越低 = 模型越确信
- 公式：`PPL = exp(loss)`
- 用法：`prob = 1 / PPL`

### 数据划分
- 前2000条：测试集
- 2000后：示例库

### 相似度计算
- 模型：all-MiniLM-L6-v2
- 方法：余弦相似度
- 结果：corpus_idx.txt

---

## ✅ 快速检查清单

### 运行前
- [ ] Python 3.7+
- [ ] GPU 显存 ≥ 16GB
- [ ] 已安装依赖
- [ ] 已下载 counterfact.json

### 理解项目
- [ ] 知道什么是上下文学习
- [ ] 知道三个评估指标
- [ ] 知道三种示例类型

### 代码理解
- [ ] 知道每个文件的作用
- [ ] 理解 construct_icl_examples()
- [ ] 理解 icl_lm_eval()

---

## 🚀 快速开始（3步）

```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 清理数据
python clean_paraphrase.py

# 3. 运行实验（使用小模型测试）
python icl.py --model_name gpt2-xl
```

---

打印这张卡片，放在手边，随时查看！📌

